# Environment variables for Agentic LLM Search with Python 3.12

# Model configuration - Uncomment one of the sections below:

# Option 1: Local TinyLlama configuration
DEFAULT_MODEL=./src/models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
MODEL_PROVIDER=huggingface

# Option 1a: Local Llama 3 configuration
# DEFAULT_MODEL=./src/models/llama-3-8b-instruct.Q4_K_M.gguf
# MODEL_PROVIDER=huggingface

# Option 1b: Local Phi-3 configuration
# DEFAULT_MODEL=./src/models/phi-3-mini-4k-instruct-q4_k_m.gguf
# MODEL_PROVIDER=huggingface

# Option 1c: Local Llama 2 configuration
# DEFAULT_MODEL=./src/models/llama-2-7b-chat.Q4_K_M.gguf
# MODEL_PROVIDER=huggingface

# Option 2: Azure OpenAI configuration (uncomment to use)
# DEFAULT_MODEL=gpt-35-turbo
# MODEL_PROVIDER=azure-openai
# The deployment name from Azure OpenAI - must match exactly
AZURE_OPENAI_DEPLOYMENT=your-deployment-name-here
TEMPERATURE=0.7
MAX_TOKENS=2000

# Optional API keys (if using cloud models)
# OPENAI_API_KEY=your-openai-api-key
# ANTHROPIC_API_KEY=your-anthropic-api-key

# Azure OpenAI Configuration
AZURE_OPENAI_API_KEY=your-azure-openai-api-key
AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com
AZURE_OPENAI_API_VERSION=2023-05-15



# Search configuration
SEARCH_ENGINE=duckduckgo
MAX_SEARCH_RESULTS=3  # Lower for faster results
MAX_CONTENT_LENGTH=2000

# Application settings
DEBUG=True
LOG_LEVEL=INFO

# Hardware acceleration settings
USE_GPU=True           # Set to False to force CPU only
USE_METAL=True         # For Apple Silicon (M1/M2/M3) GPUs
CONTEXT_LENGTH=4096    # Increased context length for models
GPU_LAYERS=32          # Number of layers to offload to GPU
