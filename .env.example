# Environment variables for Agentic LLM Search with Python 3.12
# NOTE: For local TinyLlama model, no API key is required

# Model configuration (Default: local TinyLlama)
DEFAULT_MODEL=./src/models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
MODEL_PROVIDER=huggingface  # 'huggingface' for local, 'openai' for cloud
TEMPERATURE=0.7
MAX_TOKENS=2000

# Optional API keys (if using cloud models)

# Azure OpenAI Configuration
AZURE_OPENAI_API_KEY=         #  Can be the same as OPENAI_API_KEY
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_VERSION=
 
# The deployment name from Azure OpenAI - must match exactly
AZURE_OPENAI_DEPLOYMENT=
TEMPERATURE=0.7
MAX_TOKENS=2000



# Search configuration
SEARCH_ENGINE=duckduckgo
MAX_SEARCH_RESULTS=3  # Lower for faster results
MAX_CONTENT_LENGTH=2000

# Application settings
DEBUG=True
LOG_LEVEL=INFO

# Hardware acceleration settings
USE_GPU=True           # Set to False to force CPU only
USE_METAL=True         # For Apple Silicon (M1/M2/M3) GPUs
CONTEXT_LENGTH=4096    # Increased context length for models
CT_METAL_LAYERS=32     # Number of layers to offload to Metal GPU
